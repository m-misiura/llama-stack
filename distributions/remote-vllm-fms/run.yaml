version: '2'
image_name: remote-vllm-fms
apis:
  - inference
  - safety

providers:
  inference:
    - provider_id: vllm-inference
      provider_type: remote::vllm
      config:
        url: ${env.VLLM_URL}
        max_tokens: ${env.MAX_TOKENS:4096}
        api_token: ${env.VLLM_API_TOKEN}
        model_name: ${env.INFERENCE_MODEL}
  
  safety:
    - provider_id: fms-safety
      provider_type: remote::fms
      config:
        detectors:
          regex:
            detector_id: regex
            base_url: ${env.FMS_REGEX_URL}
            is_chat: false
            use_orchestrator_api: false
            confidence_threshold: 0.5
            detector_params:
              regex: ["email"]
            message_types: ["user", "system"]

metadata_store:
  type: sqlite
  db_path: ${env.SQLITE_STORE_DIR:~/.llama/distributions/remote-vllm-fms}/registry.db

models:
  - model_id: "meta-llama/Llama-3.1-8B-Instruct"
    provider_id: vllm-inference
    name: "Llama 3.1 8B Instruct"
    description: "Llama language model for text generation"

server:
  port: 5001
  tls_certfile: null
  tls_keyfile: null