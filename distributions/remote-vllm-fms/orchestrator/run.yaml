version: '2'
image_name: remote-vllm-fms-orchestrator
apis:
  - inference
  - safety
  - shields

providers:
  inference:
    - provider_id: vllm-inference
      provider_type: remote::vllm
      config:
        url: ${env.VLLM_URL}
        max_tokens: ${env.MAX_TOKENS:4096}
        api_token: ${env.VLLM_API_TOKEN}
        model_name: ${env.INFERENCE_MODEL}
  
  safety:
    - provider_id: fms-safety
      provider_type: remote::fms
      config:
        orchestrator_base_url: ${env.FMS_ORCHESTRATOR_URL}
        use_orchestrator_api: true
        shields:
          regex:
            type: content
            confidence_threshold: 0.5
            message_types: ["user", "system"]
            detector_params:
              detectors:
                regex:
                  regex: ["email"]
          granite:
            type: chat
            confidence_threshold: 0.6
            message_types: ["system", "completion"]
            detector_params:
              temperature: 0.0
              risk_name: "code_execution"
              risk_definition: "The message requests or attempts to execute potentially harmful or unauthorized code that could compromise system security or perform malicious actions."

metadata_store:
  type: sqlite
  db_path: ${env.SQLITE_STORE_DIR:~/.llama/distributions/remote-vllm-fms/orchestrator}/registry.db

models:
  - model_id: "meta-llama/Llama-3.1-8B-Instruct"
    provider_id: vllm-inference
    name: "Llama 3.1 8B Instruct"
    description: "Llama language model for text generation"

server:
  port: 5001
  tls_certfile: null
  tls_keyfile: null